Initial notes

Plan: 
Raschkas LLM book; How to build an LLM from scratch
RAG: Retrieval-augmented generation; vectorize parts of pdfs

# Attention mechanism - self-attention
Self-attention is designed to enhance input representations
by enabling each position in a sequence to engange
with and determine the relevance of every other position
within the same sequence. 

"Attending" to different parts of the input. 

For an input sequence x_1 to x_t (such as text):
- convert this into a token embedding (see embeddings_test.py)
- For the phrase "i love you", the word "i" is now a vector
embedding x_1, "love" is vector x_2 ... etc. 

The goal: compute a context vector z_i for each input sequence element
x_i in x_1 to x_t, where z and x have the same dimensions. 
- z_i is a weighted sum over inputs x_1 to x_t
- it is context specific to input
- the vector z_2 is a weighted sum of all inputs x_1 to x_t, and
it's weighted with respect to x_2 (the token embedding vector for word "love").

The attention weights are the weights that determine how much each of the input elelemnts 
contributes to the weighted sum when computing z_i. 
- z_i can be considered as a modified version of x_i, with information about all other 
input elements. 


How to: find the context vector z

1. chose an x_i to find z_i 
2. for each x_i: 
    omega_i,j = dot-product of x_i and all other x-vectors
    - where j is the other vector x_j 

z_i is a new vector with len(z_i) = len(x_i), and it 
contains elements omega_i,j where j = len(z_i). 

However - this is not the final z_i, rather the unnormalized attention scores. 
We normalize then with a built-in softmax (essensially omega_j,i) so
that it sums up to 1. The result is a set of attention weights 
a_i,j where a_i,j = normalized(w_i,j)


## Buffers in PyTorch

self.register_buffer(x)

Sometimes, masks made with the torch.triu method get stuck on the cpu 
as you try to transfer your parameters to the gpu/cuda using the
the to_cuda command. Tensors generally are not automatically tranferred. 

We could say nn.Parameter rather than torch.triu, but the norm is that 
parameters contain trainable elements. This is why we want to use the 
register_buffer method in the nn.Module class of pytorch because it will 
follow the transfer from cpu to gpu. 

Remeber: device issues may be due to cpu vs. gpu issues and can be solved 
by registering the buffer with the module, although the type will remain a 
tensor, device will change. 


## Text generation: how to measure loss?
Generally we use the cross-entropy loss function. If we have 
2 input examples in a batch containing 3 tokens each and a 
token dimension of 50k, we have a 2,3, 50k matrix for our two input
examples, 6 words in total. The 50k dimension represents our vocabulary
size, and if we apply the softmax function to the tensor, we get
the probability score for all tokens/word in the vocab, pr. token. 

For example the word "life" would likely have a high probablilty 
on the long tensor of probabilities found after the softmax is applied. 

But apparently not, as the example illustrates how the correct
word only as a probability of 0.6. Anyhow, using the argmax
function will select the highest probability and use that. 

